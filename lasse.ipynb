{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 11:30:06.824428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-15 11:30:07.745446: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ts/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-15 11:30:07.745508: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-15 11:30:15.491311: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ts/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-15 11:30:15.491792: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/ts/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-12-15 11:30:15.491821: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "#import os\n",
    "#from PIL import Image as im\n",
    "import cv2\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import torchdata as td\n",
    "#from torchmetrics.functional import jaccard_index\n",
    "#from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image as im\n",
    "from skimage.measure import label, regionprops\n",
    "#from sklearn.metrics import f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "#from keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = pickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "def save_zipped_pickle(obj, filename):\n",
    "    with gzip.open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Read In\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_zipped_pickle(\"train.pkl\")\n",
    "test_data = load_zipped_pickle(\"test.pkl\")\n",
    "sample_data = load_zipped_pickle(\"sample.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max video length is 334 so can add padding to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "334\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "maximum_train = 0\n",
    "print(len(train_data))\n",
    "for i in range(len(train_data)):\n",
    "    if(train_data[i][\"video\"].shape[2] > maximum_train):\n",
    "        maximum_train = train_data[i][\"video\"].shape[2]\n",
    "\n",
    "maximum_test = 0\n",
    "for i in range(len(test_data)):\n",
    "    if(test_data[i][\"video\"].shape[2] > maximum_test):\n",
    "        maximum_test = test_data[i][\"video\"].shape[2]\n",
    "\n",
    "\n",
    "print(maximum_train)\n",
    "print(maximum_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amateur:  46\n",
      "expert:  19\n"
     ]
    }
   ],
   "source": [
    "amateur = 0\n",
    "expert = 0\n",
    "for i in range(len(train_data)):\n",
    "    if train_data[i][\"dataset\"] == \"amateur\":\n",
    "        amateur += 1\n",
    "    else:\n",
    "        expert += 1\n",
    "\n",
    "print(\"amateur: \", amateur)\n",
    "print(\"expert: \", expert)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "### First approach: Videos have different lengths so consider individual images - each with the same box as label - and increase batch size â€“ didn't really work\n",
    "### Second approach: Create an array of image sequences which are fed to the neural network at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 112, 334)\n",
      "(1, 112, 112, 334)\n"
     ]
    }
   ],
   "source": [
    "x = np.array(train_data[0][\"video\"])\n",
    "print(x.shape)\n",
    "print(x[None,...].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [36], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(video_array) \u001b[39m<\u001b[39m \u001b[39m334\u001b[39m:\n\u001b[1;32m     12\u001b[0m     video_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mpad(video_array, ((\u001b[39m0\u001b[39m, \u001b[39m334\u001b[39m\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(video_array)), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m)), \u001b[39m'\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m video_array \u001b[39m=\u001b[39m video_array[\u001b[39mNone\u001b[39;49;00m,\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m.\u001b[39;49m]\n\u001b[1;32m     14\u001b[0m x_train_box\u001b[39m.\u001b[39mappend(video_array)\n\u001b[1;32m     15\u001b[0m y_train_box\u001b[39m.\u001b[39mappend(train_data[i][\u001b[39m\"\u001b[39m\u001b[39mbox\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Second approach\n",
    "x_train_box = []\n",
    "y_train_box = []\n",
    "x_test_box = []\n",
    "\n",
    "for i in range(len(train_data)):\n",
    "    video_array = []\n",
    "    for j in range(train_data[i][\"video\"].shape[2]):\n",
    "        frame = cv2.resize(train_data[i][\"video\"][:,:,j], (256, 256))\n",
    "        video_array.append(frame)\n",
    "    if len(video_array) < 334:\n",
    "        video_array = np.pad(video_array, ((0, 334-len(video_array)), (0, 0), (0, 0)), 'constant')\n",
    "    video_array = video_array[None,...]\n",
    "    x_train_box.append(video_array)\n",
    "    y_train_box.append(train_data[i][\"box\"])\n",
    "\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    video_array = []\n",
    "    for j in range(test_data[i][\"video\"].shape[2]):\n",
    "        frame = cv2.resize(test_data[i][\"video\"][:,:,j], (256, 256))\n",
    "        video_array.append(frame)\n",
    "    if len(video_array) < 334:\n",
    "        video_array = np.pad(video_array, ((0, 334-len(video_array)), (0, 0), (0, 0)), 'constant')\n",
    "    x_test_box.append(video_array)\n",
    "\n",
    "x_train_box = np.array(x_train_box, dtype=(\"object\"))\n",
    "y_train_box = np.array(y_train_box, dtype=(\"object\"))\n",
    "x_test_box = np.array(x_test_box, dtype=(\"object\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split training set into training and validation set\n",
    "split  80/20 (52/13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val_box = x_train_box[52:,:,:,:]\n",
    "x_train_box = x_train_box[:52,:,:,:]\n",
    "y_val_box = y_train_box[52:]\n",
    "y_train_box = y_train_box[:52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 334, 256, 256)\n",
      "(13, 334, 256, 256)\n",
      "(52,)\n",
      "(13,)\n",
      "(20, 334, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_box.shape)\n",
    "print(x_val_box.shape)\n",
    "print(y_train_box.shape)\n",
    "print(y_val_box.shape)\n",
    "print(x_test_box.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence lengths:  [334, 177, 195, 181, 211, 209, 129, 179, 168, 151, 113, 214, 151, 166, 210, 118, 177, 171, 258, 173, 190, 109, 206, 196, 114, 176, 177, 190, 201, 178, 128, 215, 174, 221, 150, 179, 141, 140, 184, 159, 154, 172, 155, 216, 162, 198, 76, 83, 54, 90, 94, 116, 71, 85, 70, 79, 75, 95, 101, 113, 159, 63, 84, 126, 65]\n",
      "Sum:  9869\n",
      "expert length:  1186\n",
      "expert percentage:  0.12017428310872429\n",
      "52.0\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = []\n",
    "for i in range(len(train_data)):\n",
    "    sequence_lengths.append(train_data[i][\"video\"].shape[2])\n",
    "print(\"sequence lengths: \", sequence_lengths)\n",
    "# Sum = np.sum(sequence_lengths)\n",
    "# print(\"Sum: \", Sum)\n",
    "# expert_length = np.sum(sequence_lengths[52:])\n",
    "# print(\"expert length: \", expert_length)\n",
    "# print(\"expert percentage: \", expert_length/Sum)\n",
    "# print(0.8*65)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture: Unet with a LSTM layer between encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv_block_box(x, n_filters):\n",
    "   x = layers.Conv2D(n_filters, 3, padding=\"same\")(x)#kernel_initializer = \"he_normal\"\n",
    "   x = layers.BatchNormalization()(x)\n",
    "   x = layers.ReLU()(x)\n",
    "   x = layers.Conv2D(n_filters, 3, padding=\"same\")(x)#kernel_initializer = \"he_normal\"\n",
    "   x = layers.BatchNormalization()(x)\n",
    "   x = layers.ReLU()(x)\n",
    "   return x\n",
    "\n",
    "def downsample_block_box(x, n_filters):\n",
    "   f = double_conv_block_box(x, n_filters)\n",
    "   p = layers.MaxPool2D(2)(f)\n",
    "   #p = layers.Dropout(0.3)(p)\n",
    "   return f, p\n",
    "\n",
    "def upsample_block_box(x, conv_features, n_filters):\n",
    "   x = layers.Conv2DTranspose(n_filters, 2, 2, padding=\"valid\")(x)\n",
    "   x = layers.concatenate([x, conv_features])\n",
    "   #x = layers.Dropout(0.3)(x)\n",
    "   x = double_conv_block_box(x, n_filters)\n",
    "   return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(img_size):\n",
    "    inputs = layers.Input(shape=(334, img_size, img_size))\n",
    "    # Make Unet with LSTM layer between encoder and decoder\n",
    "    # Encoder\n",
    "    f1, p1 = downsample_block_box(inputs, 16)\n",
    "    f2, p2 = downsample_block_box(p1, 32)\n",
    "    f3, p3 = downsample_block_box(p2, 64)\n",
    "\n",
    "    # LSTM layer for bottleneck\n",
    "    b1 = layers.LSTM(128, return_sequences=True)(p3)\n",
    "\n",
    "    # Decoder\n",
    "    u1 = upsample_block_box(b1, f3, 64)\n",
    "    u2 = upsample_block_box(u1, f2, 32)\n",
    "    u3 = upsample_block_box(u2, f1, 16)\n",
    "\n",
    "    # Output\n",
    "    outputs = layers.Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(u3)\n",
    "    model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seed\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"lstm_2\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 41, 32, 64)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m      2\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> 5\u001b[0m model \u001b[39m=\u001b[39m create_model(\u001b[39m256\u001b[39;49m)\n\u001b[1;32m      6\u001b[0m model\u001b[39m.\u001b[39msummary()\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m),\n\u001b[1;32m     10\u001b[0m     \u001b[39m#loss=keras.losses.CategoricalCrossentropy(),\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     loss\u001b[39m=\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mBinaryCrossentropy(),\n\u001b[1;32m     12\u001b[0m     metrics\u001b[39m=\u001b[39m[keras\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39mBinaryAccuracy(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[1;32m     13\u001b[0m )\n",
      "Cell \u001b[0;32mIn [46], line 10\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(img_size)\u001b[0m\n\u001b[1;32m      7\u001b[0m f3, p3 \u001b[39m=\u001b[39m downsample_block_box(p2, \u001b[39m64\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# LSTM layer for bottleneck\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m b1 \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39;49mLSTM(\u001b[39m128\u001b[39;49m, return_sequences\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)(p3)\n\u001b[1;32m     12\u001b[0m \u001b[39m# Decoder\u001b[39;00m\n\u001b[1;32m     13\u001b[0m u1 \u001b[39m=\u001b[39m upsample_block_box(b1, f3, \u001b[39m64\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/layers/rnn/base_rnn.py:556\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m inputs, initial_state, constants \u001b[39m=\u001b[39m rnn_utils\u001b[39m.\u001b[39mstandardize_args(\n\u001b[1;32m    552\u001b[0m     inputs, initial_state, constants, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_constants\n\u001b[1;32m    553\u001b[0m )\n\u001b[1;32m    555\u001b[0m \u001b[39mif\u001b[39;00m initial_state \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m constants \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# If any of `initial_state` or `constants` are specified and are Keras\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m# tensors, then add them to the inputs and temporarily modify the\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39m# input_spec to include them.\u001b[39;00m\n\u001b[1;32m    562\u001b[0m additional_inputs \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/input_spec.py:232\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    230\u001b[0m     ndim \u001b[39m=\u001b[39m shape\u001b[39m.\u001b[39mrank\n\u001b[1;32m    231\u001b[0m     \u001b[39mif\u001b[39;00m ndim \u001b[39m!=\u001b[39m spec\u001b[39m.\u001b[39mndim:\n\u001b[0;32m--> 232\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    233\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00minput_index\u001b[39m}\u001b[39;00m\u001b[39m of layer \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    234\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mis incompatible with the layer: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexpected ndim=\u001b[39m\u001b[39m{\u001b[39;00mspec\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m, found ndim=\u001b[39m\u001b[39m{\u001b[39;00mndim\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFull shape received: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(shape)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39mmax_ndim \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     ndim \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm_2\" is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: (None, 41, 32, 64)"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "EPOCHS = 10\n",
    "\n",
    "\n",
    "model = create_model(256)\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    #loss=keras.losses.CategoricalCrossentropy(),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name='accuracy')]\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train_box,\n",
    "    y_train_box,\n",
    "    validation_data=(x_val_box, y_val_box),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_box.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model if already trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model_box.h5\n",
    "# model = tf.keras.models.load_model(\"model_box.h5\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62/62 [==============================] - 54s 870ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1974, 256, 256, 1)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_train_box[test_idx])\n",
    "x_train_box[test_idx].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get The video lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 333,  510,  705,  886, 1097, 1306, 1435, 1614, 1782, 1933, 2046,\n",
       "       2260, 2411, 2577, 2787, 2905, 3082, 3253, 3511, 3684, 3874, 3983,\n",
       "       4189, 4385, 4499, 4675, 4852, 5042, 5243, 5421, 5549, 5764, 5938,\n",
       "       6159, 6309, 6488, 6629, 6769, 6953, 7112, 7266, 7438, 7593, 7809,\n",
       "       7971, 8169, 8245, 8328, 8382, 8472, 8566, 8682, 8753, 8838, 8908,\n",
       "       8987, 9062, 9157, 9258, 9371, 9530, 9593, 9677, 9803, 9868])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_lengths = [dic[\"video\"].shape[2] for dic in train_data]\n",
    "# make video_lengths entries sum up preceding entries\n",
    "video_lengths = np.cumsum(video_lengths) - 1\n",
    "video_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_frame(indexes):\n",
    "    videos = []\n",
    "    frames = []\n",
    "    for index in indexes:\n",
    "        for i, frame in enumerate(video_lengths):\n",
    "            if index <= frame:\n",
    "                videos.append(i)\n",
    "                if(i == 0):\n",
    "                    frames.append(index)\n",
    "                else:\n",
    "                    frames.append(index - video_lengths[i-1] - 1)\n",
    "                break\n",
    "    return videos, frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos, frames = get_video_frame(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TH = 0.999\n",
    "\n",
    "for i in range(10):\n",
    "    gt =train_data[videos[i]][\"video\"][:,:,frames[i]]\n",
    "    plt.imshow(gt)\n",
    "    plt.show()\n",
    "    first_frame = 255 * pred[i,:,:,0]\n",
    "    first_frame = first_frame > (255 * TH)\n",
    "    plt.imshow(first_frame)\n",
    "    plt.show()\n",
    "\n",
    "# test_frame = 255 * pred[0,:,:,0]\n",
    "# test_frame = test_frame > (255 * TH)\n",
    "# print(test_frame)\n",
    "# plt.imshow(test_frame)\n",
    "# plt.show()\n",
    "\n",
    "# pp = cv2.resize(255 * pred[0,:,:], dsize=train_data[0][\"video\"][0].shape[::-1])\n",
    "# pp = pp > (255 * TH)    \n",
    "# pred_img = im.fromarray(pp)\n",
    "# im\n",
    "# plt.imshow(y_train_box[test_idx][0][:,:,0])\n",
    "# plt.show()\n",
    "# plt.imshow(pred[0][:,:,0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_train_box[test_idx])\n",
    "pred = np.squeeze(pred)\n",
    "\n",
    "TH = 0.999\n",
    "\n",
    "NB_OF_AREAS = 4\n",
    "intersection = 0\n",
    "union = 0\n",
    "fehlt = 0\n",
    "for i in range(39):\n",
    "    idx = test_idx[i]\n",
    "    ff = train_data[idx//3][\"frames\"][idx%3]\n",
    "    gt = train_data[idx//3][\"label\"][:,:,ff]\n",
    "    gt_img = im.fromarray(gt)\n",
    "    \n",
    "    pp = cv2.resize(255 * pred[i,:,:], dsize=gt.shape[::-1])\n",
    "    pp = pp > (255 * TH)    \n",
    "    pred_img = im.fromarray(pp)\n",
    "    \n",
    "    lab = label(pp)\n",
    "    rps = regionprops(lab)\n",
    "    area_idx = np.argsort([r.area for r in rps])[::-1]\n",
    "    new_pp = np.zeros_like(pp)\n",
    "    for j in area_idx[:NB_OF_AREAS]:\n",
    "        new_pp[tuple(rps[j].coords.T)] = True\n",
    "    new_pred_img = im.fromarray(new_pp)\n",
    "    \n",
    "    fehlt += np.count_nonzero(np.logical_and(gt, np.logical_not(new_pp)))\n",
    "    intersection += np.count_nonzero(np.logical_and(gt, new_pp))\n",
    "    union += np.count_nonzero(np.logical_or(gt, new_pp))\n",
    "    \n",
    "print(fehlt)\n",
    "print(\"score:\")\n",
    "print(intersection / union)\n",
    "\n",
    "# pred = im.fromarray((np.squeeze(model.predict(x_train[16:17]))>0.8))\n",
    "# gt = im.fromarray(cv2.resize(255 * train_data[5][\"label\"][:,:,51].astype(np.ubyte), dsize=(360, 360)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
